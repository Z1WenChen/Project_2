{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bad4f69-e842-452a-860a-c50ad6716664",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from finta import TA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0739e20-c74e-46d0-9582-24c3230d93db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1768"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#yahoo finance stock data (for longer timeframe)\n",
    "import yfinance as yf\n",
    "\n",
    "def stock_df(ticker, start, end):\n",
    "    stock = yf.Ticker(ticker)\n",
    "    stock_df = stock.history(start = start, end = end)\n",
    "    return stock_df\n",
    "\n",
    "start = pd.to_datetime('2015-01-01')\n",
    "end = pd.to_datetime('today')\n",
    "                     \n",
    "spy_df = stock_df('SPY', start, end)\n",
    "\n",
    "len(spy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3fe1b4f-1278-4af9-ad4c-5cb8577f775a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Stock Splits</th>\n",
       "      <th>Actual Return</th>\n",
       "      <th>Profit/Loss</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-05</th>\n",
       "      <td>179.236555</td>\n",
       "      <td>179.412128</td>\n",
       "      <td>176.760943</td>\n",
       "      <td>177.085754</td>\n",
       "      <td>169632600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.018060</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-06</th>\n",
       "      <td>177.410530</td>\n",
       "      <td>177.963598</td>\n",
       "      <td>174.574986</td>\n",
       "      <td>175.417755</td>\n",
       "      <td>209151400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.009419</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-07</th>\n",
       "      <td>176.822402</td>\n",
       "      <td>177.963648</td>\n",
       "      <td>176.348353</td>\n",
       "      <td>177.603714</td>\n",
       "      <td>125346700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012461</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-08</th>\n",
       "      <td>179.096043</td>\n",
       "      <td>180.983491</td>\n",
       "      <td>179.078495</td>\n",
       "      <td>180.755234</td>\n",
       "      <td>147217800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.017745</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-09</th>\n",
       "      <td>181.194212</td>\n",
       "      <td>181.211774</td>\n",
       "      <td>178.657143</td>\n",
       "      <td>179.306778</td>\n",
       "      <td>158567300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.008013</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-12</th>\n",
       "      <td>179.447221</td>\n",
       "      <td>179.614020</td>\n",
       "      <td>177.261298</td>\n",
       "      <td>177.902145</td>\n",
       "      <td>144396100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.007834</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-13</th>\n",
       "      <td>179.192631</td>\n",
       "      <td>180.386547</td>\n",
       "      <td>176.023488</td>\n",
       "      <td>177.401764</td>\n",
       "      <td>214553300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.002813</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-14</th>\n",
       "      <td>175.268510</td>\n",
       "      <td>176.541445</td>\n",
       "      <td>174.320413</td>\n",
       "      <td>176.330750</td>\n",
       "      <td>192991100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.006037</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-15</th>\n",
       "      <td>177.006703</td>\n",
       "      <td>177.340288</td>\n",
       "      <td>174.592536</td>\n",
       "      <td>174.715439</td>\n",
       "      <td>176613900</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.009161</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-16</th>\n",
       "      <td>174.496010</td>\n",
       "      <td>177.173544</td>\n",
       "      <td>174.302875</td>\n",
       "      <td>177.006744</td>\n",
       "      <td>211879600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.013114</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-20</th>\n",
       "      <td>177.682707</td>\n",
       "      <td>177.963635</td>\n",
       "      <td>175.725041</td>\n",
       "      <td>177.384232</td>\n",
       "      <td>130991100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-21</th>\n",
       "      <td>176.892598</td>\n",
       "      <td>178.788819</td>\n",
       "      <td>176.400988</td>\n",
       "      <td>178.279648</td>\n",
       "      <td>122942700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-22</th>\n",
       "      <td>179.078507</td>\n",
       "      <td>181.071282</td>\n",
       "      <td>177.621225</td>\n",
       "      <td>180.930832</td>\n",
       "      <td>174356000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.014871</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-23</th>\n",
       "      <td>180.658681</td>\n",
       "      <td>180.930835</td>\n",
       "      <td>179.798364</td>\n",
       "      <td>179.938828</td>\n",
       "      <td>117516800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.005483</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-26</th>\n",
       "      <td>179.710577</td>\n",
       "      <td>180.456766</td>\n",
       "      <td>178.955601</td>\n",
       "      <td>180.360199</td>\n",
       "      <td>92009700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-27</th>\n",
       "      <td>178.183033</td>\n",
       "      <td>179.192588</td>\n",
       "      <td>177.103246</td>\n",
       "      <td>177.981125</td>\n",
       "      <td>134044600</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.013191</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-28</th>\n",
       "      <td>179.236536</td>\n",
       "      <td>179.341877</td>\n",
       "      <td>175.496776</td>\n",
       "      <td>175.698685</td>\n",
       "      <td>168514300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012824</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-29</th>\n",
       "      <td>175.909384</td>\n",
       "      <td>177.594909</td>\n",
       "      <td>174.416979</td>\n",
       "      <td>177.322769</td>\n",
       "      <td>173585400</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-30</th>\n",
       "      <td>176.076159</td>\n",
       "      <td>177.480757</td>\n",
       "      <td>174.812012</td>\n",
       "      <td>175.092926</td>\n",
       "      <td>197729700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.012575</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-02</th>\n",
       "      <td>175.619677</td>\n",
       "      <td>177.357874</td>\n",
       "      <td>173.697121</td>\n",
       "      <td>177.261307</td>\n",
       "      <td>163107000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.012384</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close     Volume  \\\n",
       "Date                                                                    \n",
       "2015-01-05  179.236555  179.412128  176.760943  177.085754  169632600   \n",
       "2015-01-06  177.410530  177.963598  174.574986  175.417755  209151400   \n",
       "2015-01-07  176.822402  177.963648  176.348353  177.603714  125346700   \n",
       "2015-01-08  179.096043  180.983491  179.078495  180.755234  147217800   \n",
       "2015-01-09  181.194212  181.211774  178.657143  179.306778  158567300   \n",
       "2015-01-12  179.447221  179.614020  177.261298  177.902145  144396100   \n",
       "2015-01-13  179.192631  180.386547  176.023488  177.401764  214553300   \n",
       "2015-01-14  175.268510  176.541445  174.320413  176.330750  192991100   \n",
       "2015-01-15  177.006703  177.340288  174.592536  174.715439  176613900   \n",
       "2015-01-16  174.496010  177.173544  174.302875  177.006744  211879600   \n",
       "2015-01-20  177.682707  177.963635  175.725041  177.384232  130991100   \n",
       "2015-01-21  176.892598  178.788819  176.400988  178.279648  122942700   \n",
       "2015-01-22  179.078507  181.071282  177.621225  180.930832  174356000   \n",
       "2015-01-23  180.658681  180.930835  179.798364  179.938828  117516800   \n",
       "2015-01-26  179.710577  180.456766  178.955601  180.360199   92009700   \n",
       "2015-01-27  178.183033  179.192588  177.103246  177.981125  134044600   \n",
       "2015-01-28  179.236536  179.341877  175.496776  175.698685  168514300   \n",
       "2015-01-29  175.909384  177.594909  174.416979  177.322769  173585400   \n",
       "2015-01-30  176.076159  177.480757  174.812012  175.092926  197729700   \n",
       "2015-02-02  175.619677  177.357874  173.697121  177.261307  163107000   \n",
       "\n",
       "            Dividends  Stock Splits  Actual Return  Profit/Loss  \n",
       "Date                                                             \n",
       "2015-01-05        0.0             0      -0.018060          0.0  \n",
       "2015-01-06        0.0             0      -0.009419          0.0  \n",
       "2015-01-07        0.0             0       0.012461          1.0  \n",
       "2015-01-08        0.0             0       0.017745          1.0  \n",
       "2015-01-09        0.0             0      -0.008013          0.0  \n",
       "2015-01-12        0.0             0      -0.007834          0.0  \n",
       "2015-01-13        0.0             0      -0.002813          0.0  \n",
       "2015-01-14        0.0             0      -0.006037          0.0  \n",
       "2015-01-15        0.0             0      -0.009161          0.0  \n",
       "2015-01-16        0.0             0       0.013114          1.0  \n",
       "2015-01-20        0.0             0       0.002133          1.0  \n",
       "2015-01-21        0.0             0       0.005048          1.0  \n",
       "2015-01-22        0.0             0       0.014871          1.0  \n",
       "2015-01-23        0.0             0      -0.005483          0.0  \n",
       "2015-01-26        0.0             0       0.002342          1.0  \n",
       "2015-01-27        0.0             0      -0.013191          0.0  \n",
       "2015-01-28        0.0             0      -0.012824          0.0  \n",
       "2015-01-29        0.0             0       0.009244          1.0  \n",
       "2015-01-30        0.0             0      -0.012575          0.0  \n",
       "2015-02-02        0.0             0       0.012384          1.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spy_df[\"Monetary Gain\"] = spy_df[\"Close\"].diff()\n",
    "spy_df['Actual Return'] = spy_df[\"Close\"].pct_change()\n",
    "\n",
    "spy_df.loc[(spy_df['Actual Return'] >= 0), 'Profit/Loss'] = 1\n",
    "spy_df.loc[(spy_df['Actual Return'] < 0), 'Profit/Loss'] = 0\n",
    "\n",
    "# spy_df['Trades'] = np.abs(spy_df['Trading Signal'].diff())\n",
    "\n",
    "# spy_df['Strategy Returns'] = spy_df['Actual Return'] * spy_df['Trading Signal'].shift()\n",
    "\n",
    "spy_df.dropna(inplace= True)\n",
    "\n",
    "spy_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e72fe9bb-72ea-415c-b406-c8bd89036499",
   "metadata": {},
   "outputs": [],
   "source": [
    "spy_momentum = pd.DataFrame()\n",
    "spy_momentum_changes = pd.DataFrame()\n",
    "spy_momentum_changes_lag = pd.DataFrame()\n",
    "\n",
    "spy_trend = pd.DataFrame()\n",
    "spy_trend_changes = pd.DataFrame()\n",
    "\n",
    "spy_volatility = pd.DataFrame()\n",
    "spy_volatility_changes = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e09fe22-be8a-43c8-881f-a37bca9961a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RSI</th>\n",
       "      <th>CCI</th>\n",
       "      <th>ROC</th>\n",
       "      <th>STO</th>\n",
       "      <th>Closes Up/Down</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-23</th>\n",
       "      <td>59.273367</td>\n",
       "      <td>119.925394</td>\n",
       "      <td>2.577317</td>\n",
       "      <td>81.575265</td>\n",
       "      <td>-0.005483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-26</th>\n",
       "      <td>60.646437</td>\n",
       "      <td>87.284482</td>\n",
       "      <td>1.552042</td>\n",
       "      <td>87.674233</td>\n",
       "      <td>0.002342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-27</th>\n",
       "      <td>50.329233</td>\n",
       "      <td>-2.146055</td>\n",
       "      <td>-1.534732</td>\n",
       "      <td>53.239305</td>\n",
       "      <td>-0.013191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-28</th>\n",
       "      <td>42.805514</td>\n",
       "      <td>-60.736293</td>\n",
       "      <td>-2.012246</td>\n",
       "      <td>20.203065</td>\n",
       "      <td>-0.012824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-29</th>\n",
       "      <td>48.683921</td>\n",
       "      <td>-70.027943</td>\n",
       "      <td>-0.325671</td>\n",
       "      <td>43.710206</td>\n",
       "      <td>0.009244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>61.105538</td>\n",
       "      <td>74.890025</td>\n",
       "      <td>1.867468</td>\n",
       "      <td>95.369673</td>\n",
       "      <td>0.005790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>60.893112</td>\n",
       "      <td>76.400896</td>\n",
       "      <td>2.739358</td>\n",
       "      <td>91.574121</td>\n",
       "      <td>-0.000335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>50.135922</td>\n",
       "      <td>11.190858</td>\n",
       "      <td>1.850525</td>\n",
       "      <td>59.778060</td>\n",
       "      <td>-0.019202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>49.682393</td>\n",
       "      <td>-27.253761</td>\n",
       "      <td>2.848475</td>\n",
       "      <td>58.252392</td>\n",
       "      <td>-0.000939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>47.727484</td>\n",
       "      <td>-50.727289</td>\n",
       "      <td>0.654343</td>\n",
       "      <td>51.837668</td>\n",
       "      <td>-0.003954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1754 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  RSI         CCI       ROC        STO  Closes Up/Down\n",
       "Date                                                                  \n",
       "2015-01-23  59.273367  119.925394  2.577317  81.575265       -0.005483\n",
       "2015-01-26  60.646437   87.284482  1.552042  87.674233        0.002342\n",
       "2015-01-27  50.329233   -2.146055 -1.534732  53.239305       -0.013191\n",
       "2015-01-28  42.805514  -60.736293 -2.012246  20.203065       -0.012824\n",
       "2015-01-29  48.683921  -70.027943 -0.325671  43.710206        0.009244\n",
       "...               ...         ...       ...        ...             ...\n",
       "2022-01-03  61.105538   74.890025  1.867468  95.369673        0.005790\n",
       "2022-01-04  60.893112   76.400896  2.739358  91.574121       -0.000335\n",
       "2022-01-05  50.135922   11.190858  1.850525  59.778060       -0.019202\n",
       "2022-01-06  49.682393  -27.253761  2.848475  58.252392       -0.000939\n",
       "2022-01-07  47.727484  -50.727289  0.654343  51.837668       -0.003954\n",
       "\n",
       "[1754 rows x 5 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#time period should change depending on timeframe of stock data\n",
    "\n",
    "spy_momentum[\"RSI\"] = TA.RSI(spy_df, 14)\n",
    "spy_momentum[\"CCI\"] = TA.CCI(spy_df, 14)\n",
    "spy_momentum[\"ROC\"] = TA.ROC(spy_df)\n",
    "spy_momentum[\"STO\"] = TA.STOCH(spy_df)\n",
    "spy_momentum.dropna(inplace = True)\n",
    "spy_momentum[\"Closes Up/Down\"] = spy_df[\"Actual Return\"]\n",
    "spy_momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23e43951-5411-47e2-a161-910cc3745a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RSI Diff</th>\n",
       "      <th>CCI Diff</th>\n",
       "      <th>ROC Diff</th>\n",
       "      <th>STO Diff</th>\n",
       "      <th>Daily Return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-26</th>\n",
       "      <td>1.373070</td>\n",
       "      <td>-32.640912</td>\n",
       "      <td>-1.025275</td>\n",
       "      <td>6.098968</td>\n",
       "      <td>0.002342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-27</th>\n",
       "      <td>-10.317204</td>\n",
       "      <td>-89.430537</td>\n",
       "      <td>-3.086775</td>\n",
       "      <td>-34.434928</td>\n",
       "      <td>-0.013191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-28</th>\n",
       "      <td>-7.523719</td>\n",
       "      <td>-58.590238</td>\n",
       "      <td>-0.477514</td>\n",
       "      <td>-33.036240</td>\n",
       "      <td>-0.012824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-29</th>\n",
       "      <td>5.878406</td>\n",
       "      <td>-9.291650</td>\n",
       "      <td>1.686575</td>\n",
       "      <td>23.507141</td>\n",
       "      <td>0.009244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-30</th>\n",
       "      <td>-6.422446</td>\n",
       "      <td>-18.057133</td>\n",
       "      <td>-0.975803</td>\n",
       "      <td>-32.037585</td>\n",
       "      <td>-0.012575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>2.293101</td>\n",
       "      <td>-2.834605</td>\n",
       "      <td>-0.996105</td>\n",
       "      <td>9.870788</td>\n",
       "      <td>0.005790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>-0.212426</td>\n",
       "      <td>1.510871</td>\n",
       "      <td>0.871890</td>\n",
       "      <td>-3.795552</td>\n",
       "      <td>-0.000335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>-10.757190</td>\n",
       "      <td>-65.210038</td>\n",
       "      <td>-0.888833</td>\n",
       "      <td>-31.796061</td>\n",
       "      <td>-0.019202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>-0.453528</td>\n",
       "      <td>-38.444620</td>\n",
       "      <td>0.997950</td>\n",
       "      <td>-1.525667</td>\n",
       "      <td>-0.000939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>-1.954910</td>\n",
       "      <td>-23.473528</td>\n",
       "      <td>-2.194132</td>\n",
       "      <td>-6.414724</td>\n",
       "      <td>-0.003954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1753 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             RSI Diff   CCI Diff  ROC Diff   STO Diff  Daily Return\n",
       "Date                                                               \n",
       "2015-01-26   1.373070 -32.640912 -1.025275   6.098968      0.002342\n",
       "2015-01-27 -10.317204 -89.430537 -3.086775 -34.434928     -0.013191\n",
       "2015-01-28  -7.523719 -58.590238 -0.477514 -33.036240     -0.012824\n",
       "2015-01-29   5.878406  -9.291650  1.686575  23.507141      0.009244\n",
       "2015-01-30  -6.422446 -18.057133 -0.975803 -32.037585     -0.012575\n",
       "...               ...        ...       ...        ...           ...\n",
       "2022-01-03   2.293101  -2.834605 -0.996105   9.870788      0.005790\n",
       "2022-01-04  -0.212426   1.510871  0.871890  -3.795552     -0.000335\n",
       "2022-01-05 -10.757190 -65.210038 -0.888833 -31.796061     -0.019202\n",
       "2022-01-06  -0.453528 -38.444620  0.997950  -1.525667     -0.000939\n",
       "2022-01-07  -1.954910 -23.473528 -2.194132  -6.414724     -0.003954\n",
       "\n",
       "[1753 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lag RSI Diff</th>\n",
       "      <th>Lag CCI Diff</th>\n",
       "      <th>Lag ROC Diff</th>\n",
       "      <th>Lag STO Diff</th>\n",
       "      <th>Daily Change</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-01-27</th>\n",
       "      <td>1.373070</td>\n",
       "      <td>-32.640912</td>\n",
       "      <td>-1.025275</td>\n",
       "      <td>6.098968</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-28</th>\n",
       "      <td>-10.317204</td>\n",
       "      <td>-89.430537</td>\n",
       "      <td>-3.086775</td>\n",
       "      <td>-34.434928</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-29</th>\n",
       "      <td>-7.523719</td>\n",
       "      <td>-58.590238</td>\n",
       "      <td>-0.477514</td>\n",
       "      <td>-33.036240</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-01-30</th>\n",
       "      <td>5.878406</td>\n",
       "      <td>-9.291650</td>\n",
       "      <td>1.686575</td>\n",
       "      <td>23.507141</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-02-02</th>\n",
       "      <td>-6.422446</td>\n",
       "      <td>-18.057133</td>\n",
       "      <td>-0.975803</td>\n",
       "      <td>-32.037585</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>-1.439374</td>\n",
       "      <td>-32.335915</td>\n",
       "      <td>0.449605</td>\n",
       "      <td>-4.307297</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>2.293101</td>\n",
       "      <td>-2.834605</td>\n",
       "      <td>-0.996105</td>\n",
       "      <td>9.870788</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>-0.212426</td>\n",
       "      <td>1.510871</td>\n",
       "      <td>0.871890</td>\n",
       "      <td>-3.795552</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>-10.757190</td>\n",
       "      <td>-65.210038</td>\n",
       "      <td>-0.888833</td>\n",
       "      <td>-31.796061</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>-0.453528</td>\n",
       "      <td>-38.444620</td>\n",
       "      <td>0.997950</td>\n",
       "      <td>-1.525667</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1752 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Lag RSI Diff  Lag CCI Diff  Lag ROC Diff  Lag STO Diff  \\\n",
       "Date                                                                 \n",
       "2015-01-27      1.373070    -32.640912     -1.025275      6.098968   \n",
       "2015-01-28    -10.317204    -89.430537     -3.086775    -34.434928   \n",
       "2015-01-29     -7.523719    -58.590238     -0.477514    -33.036240   \n",
       "2015-01-30      5.878406     -9.291650      1.686575     23.507141   \n",
       "2015-02-02     -6.422446    -18.057133     -0.975803    -32.037585   \n",
       "...                  ...           ...           ...           ...   \n",
       "2022-01-03     -1.439374    -32.335915      0.449605     -4.307297   \n",
       "2022-01-04      2.293101     -2.834605     -0.996105      9.870788   \n",
       "2022-01-05     -0.212426      1.510871      0.871890     -3.795552   \n",
       "2022-01-06    -10.757190    -65.210038     -0.888833    -31.796061   \n",
       "2022-01-07     -0.453528    -38.444620      0.997950     -1.525667   \n",
       "\n",
       "            Daily Change  \n",
       "Date                      \n",
       "2015-01-27           0.0  \n",
       "2015-01-28           0.0  \n",
       "2015-01-29           1.0  \n",
       "2015-01-30           0.0  \n",
       "2015-02-02           1.0  \n",
       "...                  ...  \n",
       "2022-01-03           1.0  \n",
       "2022-01-04           0.0  \n",
       "2022-01-05           0.0  \n",
       "2022-01-06           0.0  \n",
       "2022-01-07           0.0  \n",
       "\n",
       "[1752 rows x 5 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spy_momentum_changes[\"RSI Diff\"] = spy_momentum[\"RSI\"].diff()\n",
    "spy_momentum_changes[\"CCI Diff\"] = spy_momentum[\"CCI\"].diff()\n",
    "spy_momentum_changes[\"ROC Diff\"] = spy_momentum[\"ROC\"].diff()\n",
    "spy_momentum_changes[\"STO Diff\"] = spy_momentum[\"STO\"].diff()\n",
    "spy_momentum_changes.dropna(inplace = True)\n",
    "spy_momentum_changes[\"Daily Return\"] = spy_df[\"Actual Return\"]\n",
    "display(spy_momentum_changes)\n",
    "\n",
    "#Using the previous day's changes in momentum indicator to predict the pct change of the current day\n",
    "\n",
    "spy_momentum_changes_lag[\"Lag RSI Diff\"] = spy_momentum[\"RSI\"].diff().shift()\n",
    "spy_momentum_changes_lag[\"Lag CCI Diff\"] = spy_momentum[\"CCI\"].diff().shift()\n",
    "spy_momentum_changes_lag[\"Lag ROC Diff\"] = spy_momentum[\"ROC\"].diff().shift()\n",
    "spy_momentum_changes_lag[\"Lag STO Diff\"] = spy_momentum[\"STO\"].diff().shift()\n",
    "spy_momentum_changes_lag.dropna(inplace = True)\n",
    "spy_momentum_changes_lag[\"Daily Change\"] = spy_df[\"Profit/Loss\"]\n",
    "spy_momentum_changes_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03914f5b-893b-47aa-857a-cc59b3a2eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = spy_momentum_changes_lag.drop(columns = [\"Daily Change\"])\n",
    "y = spy_momentum_changes_lag[\"Daily Change\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_scaler = scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38702750-f662-4696-8a6a-ba5b5bd80efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_3 (Dense)             (None, 2)                 10        \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13\n",
      "Trainable params: 13\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "neural = Sequential()\n",
    "\n",
    "number_input_features = 4\n",
    "hidden_nodes_layer1 =  (number_input_features + 1) // 2 \n",
    "hidden_nodes_layer2 = (hidden_nodes_layer1 + 1) // 2\n",
    "\n",
    "neural.add(Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"elu\"))\n",
    "# neural.add(Dense(units=hidden_nodes_layer2, activation=\"elu\"))\n",
    "neural.add(Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "neural.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0529d123-d376-4314-ae11-b341ca10127b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6928 - accuracy: 0.5053\n",
      "Epoch 2/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6917 - accuracy: 0.5327\n",
      "Epoch 3/200\n",
      "42/42 [==============================] - 0s 559us/step - loss: 0.6910 - accuracy: 0.5479\n",
      "Epoch 4/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6902 - accuracy: 0.5457\n",
      "Epoch 5/200\n",
      "42/42 [==============================] - 0s 559us/step - loss: 0.6896 - accuracy: 0.5457\n",
      "Epoch 6/200\n",
      "42/42 [==============================] - 0s 559us/step - loss: 0.6891 - accuracy: 0.5464\n",
      "Epoch 7/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6887 - accuracy: 0.5479\n",
      "Epoch 8/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6885 - accuracy: 0.5487\n",
      "Epoch 9/200\n",
      "42/42 [==============================] - 0s 559us/step - loss: 0.6882 - accuracy: 0.5487\n",
      "Epoch 10/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6878 - accuracy: 0.5502\n",
      "Epoch 11/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6878 - accuracy: 0.5487\n",
      "Epoch 12/200\n",
      "42/42 [==============================] - 0s 624us/step - loss: 0.6876 - accuracy: 0.5510\n",
      "Epoch 13/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6875 - accuracy: 0.5533\n",
      "Epoch 14/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6873 - accuracy: 0.5502\n",
      "Epoch 15/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6872 - accuracy: 0.5510\n",
      "Epoch 16/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6871 - accuracy: 0.5518\n",
      "Epoch 17/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6872 - accuracy: 0.5487\n",
      "Epoch 18/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6870 - accuracy: 0.5464\n",
      "Epoch 19/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6869 - accuracy: 0.5472\n",
      "Epoch 20/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6869 - accuracy: 0.5479\n",
      "Epoch 21/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6868 - accuracy: 0.5487\n",
      "Epoch 22/200\n",
      "42/42 [==============================] - 0s 559us/step - loss: 0.6868 - accuracy: 0.5479\n",
      "Epoch 23/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6868 - accuracy: 0.5487\n",
      "Epoch 24/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6867 - accuracy: 0.5533\n",
      "Epoch 25/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6867 - accuracy: 0.5548\n",
      "Epoch 26/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6867 - accuracy: 0.5540\n",
      "Epoch 27/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6867 - accuracy: 0.5495\n",
      "Epoch 28/200\n",
      "42/42 [==============================] - 0s 559us/step - loss: 0.6867 - accuracy: 0.5518\n",
      "Epoch 29/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6867 - accuracy: 0.5495\n",
      "Epoch 30/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6867 - accuracy: 0.5472\n",
      "Epoch 31/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6867 - accuracy: 0.5464\n",
      "Epoch 32/200\n",
      "42/42 [==============================] - 0s 559us/step - loss: 0.6867 - accuracy: 0.5479\n",
      "Epoch 33/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6867 - accuracy: 0.5479\n",
      "Epoch 34/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6867 - accuracy: 0.5464\n",
      "Epoch 35/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6865 - accuracy: 0.5457\n",
      "Epoch 36/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6867 - accuracy: 0.5495\n",
      "Epoch 37/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6867 - accuracy: 0.5472\n",
      "Epoch 38/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6866 - accuracy: 0.5495\n",
      "Epoch 39/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6866 - accuracy: 0.5525\n",
      "Epoch 40/200\n",
      "42/42 [==============================] - 0s 559us/step - loss: 0.6866 - accuracy: 0.5586\n",
      "Epoch 41/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6866 - accuracy: 0.5548\n",
      "Epoch 42/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6866 - accuracy: 0.5556\n",
      "Epoch 43/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6865 - accuracy: 0.5563\n",
      "Epoch 44/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6866 - accuracy: 0.5556\n",
      "Epoch 45/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6866 - accuracy: 0.5556\n",
      "Epoch 46/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6866 - accuracy: 0.5586\n",
      "Epoch 47/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6865 - accuracy: 0.5563\n",
      "Epoch 48/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6866 - accuracy: 0.5556\n",
      "Epoch 49/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6865 - accuracy: 0.5578\n",
      "Epoch 50/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6866 - accuracy: 0.5594\n",
      "Epoch 51/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6866 - accuracy: 0.5578\n",
      "Epoch 52/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6866 - accuracy: 0.5594\n",
      "Epoch 53/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6866 - accuracy: 0.5601\n",
      "Epoch 54/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6865 - accuracy: 0.5594\n",
      "Epoch 55/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6865 - accuracy: 0.5571\n",
      "Epoch 56/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6865 - accuracy: 0.5563\n",
      "Epoch 57/200\n",
      "42/42 [==============================] - 0s 559us/step - loss: 0.6865 - accuracy: 0.5518\n",
      "Epoch 58/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6865 - accuracy: 0.5495\n",
      "Epoch 59/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6865 - accuracy: 0.5502\n",
      "Epoch 60/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6865 - accuracy: 0.5571\n",
      "Epoch 61/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6865 - accuracy: 0.5563\n",
      "Epoch 62/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6865 - accuracy: 0.5533\n",
      "Epoch 63/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6865 - accuracy: 0.5479\n",
      "Epoch 64/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6865 - accuracy: 0.5495\n",
      "Epoch 65/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6864 - accuracy: 0.5495\n",
      "Epoch 66/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6865 - accuracy: 0.5502\n",
      "Epoch 67/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6864 - accuracy: 0.5533\n",
      "Epoch 68/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6865 - accuracy: 0.5548\n",
      "Epoch 69/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6865 - accuracy: 0.5571\n",
      "Epoch 70/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6864 - accuracy: 0.5556\n",
      "Epoch 71/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6864 - accuracy: 0.5556\n",
      "Epoch 72/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6864 - accuracy: 0.5556\n",
      "Epoch 73/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6865 - accuracy: 0.5578\n",
      "Epoch 74/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6865 - accuracy: 0.5502\n",
      "Epoch 75/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6864 - accuracy: 0.5518\n",
      "Epoch 76/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6865 - accuracy: 0.5525\n",
      "Epoch 77/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6865 - accuracy: 0.5518\n",
      "Epoch 78/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6864 - accuracy: 0.5525\n",
      "Epoch 79/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6864 - accuracy: 0.5556\n",
      "Epoch 80/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6864 - accuracy: 0.5578\n",
      "Epoch 81/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6864 - accuracy: 0.5594\n",
      "Epoch 82/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6863 - accuracy: 0.5540\n",
      "Epoch 83/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6864 - accuracy: 0.5525\n",
      "Epoch 84/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6864 - accuracy: 0.5525\n",
      "Epoch 85/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6865 - accuracy: 0.5518\n",
      "Epoch 86/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6864 - accuracy: 0.5525\n",
      "Epoch 87/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6864 - accuracy: 0.5518\n",
      "Epoch 88/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6864 - accuracy: 0.5518\n",
      "Epoch 89/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6864 - accuracy: 0.5518\n",
      "Epoch 90/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6865 - accuracy: 0.5502\n",
      "Epoch 91/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5548\n",
      "Epoch 92/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6864 - accuracy: 0.5495\n",
      "Epoch 93/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6864 - accuracy: 0.5502\n",
      "Epoch 94/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6864 - accuracy: 0.5502\n",
      "Epoch 95/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6864 - accuracy: 0.5510\n",
      "Epoch 96/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6863 - accuracy: 0.5518\n",
      "Epoch 97/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5502\n",
      "Epoch 98/200\n",
      "42/42 [==============================] - 0s 705us/step - loss: 0.6864 - accuracy: 0.5464\n",
      "Epoch 99/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6864 - accuracy: 0.5495\n",
      "Epoch 100/200\n",
      "42/42 [==============================] - 0s 705us/step - loss: 0.6863 - accuracy: 0.5525\n",
      "Epoch 101/200\n",
      "42/42 [==============================] - 0s 706us/step - loss: 0.6864 - accuracy: 0.5586\n",
      "Epoch 102/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6863 - accuracy: 0.5556\n",
      "Epoch 103/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5548\n",
      "Epoch 104/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6863 - accuracy: 0.5578\n",
      "Epoch 105/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6863 - accuracy: 0.5571\n",
      "Epoch 106/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6864 - accuracy: 0.5563\n",
      "Epoch 107/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5563\n",
      "Epoch 108/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5586\n",
      "Epoch 109/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6863 - accuracy: 0.5578\n",
      "Epoch 110/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5571\n",
      "Epoch 111/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5556\n",
      "Epoch 112/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5548\n",
      "Epoch 113/200\n",
      "42/42 [==============================] - 0s 705us/step - loss: 0.6863 - accuracy: 0.5548\n",
      "Epoch 114/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5533\n",
      "Epoch 115/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5518\n",
      "Epoch 116/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5533\n",
      "Epoch 117/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5518\n",
      "Epoch 118/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6863 - accuracy: 0.5518\n",
      "Epoch 119/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6863 - accuracy: 0.5518\n",
      "Epoch 120/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6864 - accuracy: 0.5518\n",
      "Epoch 121/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6863 - accuracy: 0.5540\n",
      "Epoch 122/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6863 - accuracy: 0.5510\n",
      "Epoch 123/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6863 - accuracy: 0.5525\n",
      "Epoch 124/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6863 - accuracy: 0.5525\n",
      "Epoch 125/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6863 - accuracy: 0.5540\n",
      "Epoch 126/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6864 - accuracy: 0.5518\n",
      "Epoch 127/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6864 - accuracy: 0.5540\n",
      "Epoch 128/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6862 - accuracy: 0.5525\n",
      "Epoch 129/200\n",
      "42/42 [==============================] - 0s 705us/step - loss: 0.6863 - accuracy: 0.5533\n",
      "Epoch 130/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6863 - accuracy: 0.5525\n",
      "Epoch 131/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6863 - accuracy: 0.5548\n",
      "Epoch 132/200\n",
      "42/42 [==============================] - 0s 705us/step - loss: 0.6863 - accuracy: 0.5571\n",
      "Epoch 133/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6863 - accuracy: 0.5548\n",
      "Epoch 134/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6862 - accuracy: 0.5563\n",
      "Epoch 135/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6863 - accuracy: 0.5578\n",
      "Epoch 136/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6863 - accuracy: 0.5571\n",
      "Epoch 137/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6863 - accuracy: 0.5556\n",
      "Epoch 138/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6863 - accuracy: 0.5540\n",
      "Epoch 139/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6863 - accuracy: 0.5518\n",
      "Epoch 140/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 141/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6863 - accuracy: 0.5525\n",
      "Epoch 142/200\n",
      "42/42 [==============================] - 0s 997us/step - loss: 0.6862 - accuracy: 0.5548\n",
      "Epoch 143/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6863 - accuracy: 0.5525\n",
      "Epoch 144/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6862 - accuracy: 0.5510\n",
      "Epoch 145/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6862 - accuracy: 0.5510\n",
      "Epoch 146/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6863 - accuracy: 0.5510\n",
      "Epoch 147/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5518\n",
      "Epoch 148/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 149/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6863 - accuracy: 0.5533\n",
      "Epoch 150/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6862 - accuracy: 0.5525\n",
      "Epoch 151/200\n",
      "42/42 [==============================] - 0s 609us/step - loss: 0.6863 - accuracy: 0.5502\n",
      "Epoch 152/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 153/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 154/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5518\n",
      "Epoch 155/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6864 - accuracy: 0.5563\n",
      "Epoch 156/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6863 - accuracy: 0.5563\n",
      "Epoch 157/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5525\n",
      "Epoch 158/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6863 - accuracy: 0.5533\n",
      "Epoch 159/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5548\n",
      "Epoch 160/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6863 - accuracy: 0.5525\n",
      "Epoch 161/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5548\n",
      "Epoch 162/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6863 - accuracy: 0.5563\n",
      "Epoch 163/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6863 - accuracy: 0.5594\n",
      "Epoch 164/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6863 - accuracy: 0.5518\n",
      "Epoch 165/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6861 - accuracy: 0.5502\n",
      "Epoch 166/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5525\n",
      "Epoch 167/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6862 - accuracy: 0.5525\n",
      "Epoch 168/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6862 - accuracy: 0.5510\n",
      "Epoch 169/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6862 - accuracy: 0.5518\n",
      "Epoch 170/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6861 - accuracy: 0.5548\n",
      "Epoch 171/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6862 - accuracy: 0.5548\n",
      "Epoch 172/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6861 - accuracy: 0.5533\n",
      "Epoch 173/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 174/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6861 - accuracy: 0.5540\n",
      "Epoch 175/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6863 - accuracy: 0.5518\n",
      "Epoch 176/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6862 - accuracy: 0.5548\n",
      "Epoch 177/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 178/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6861 - accuracy: 0.5525\n",
      "Epoch 179/200\n",
      "42/42 [==============================] - 0s 632us/step - loss: 0.6861 - accuracy: 0.5540\n",
      "Epoch 180/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 181/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 182/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 183/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 184/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 185/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5525\n",
      "Epoch 186/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 187/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6862 - accuracy: 0.5525\n",
      "Epoch 188/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6861 - accuracy: 0.5518\n",
      "Epoch 189/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6862 - accuracy: 0.5540\n",
      "Epoch 190/200\n",
      "42/42 [==============================] - 0s 681us/step - loss: 0.6862 - accuracy: 0.5533\n",
      "Epoch 191/200\n",
      "42/42 [==============================] - 0s 803us/step - loss: 0.6862 - accuracy: 0.5578\n",
      "Epoch 192/200\n",
      "42/42 [==============================] - 0s 657us/step - loss: 0.6862 - accuracy: 0.5571\n",
      "Epoch 193/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6863 - accuracy: 0.5563\n",
      "Epoch 194/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6863 - accuracy: 0.5540\n",
      "Epoch 195/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6862 - accuracy: 0.5518\n",
      "Epoch 196/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6862 - accuracy: 0.5525\n",
      "Epoch 197/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6861 - accuracy: 0.5540\n",
      "Epoch 198/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6862 - accuracy: 0.5495\n",
      "Epoch 199/200\n",
      "42/42 [==============================] - 0s 608us/step - loss: 0.6862 - accuracy: 0.5556\n",
      "Epoch 200/200\n",
      "42/42 [==============================] - 0s 584us/step - loss: 0.6861 - accuracy: 0.5540\n"
     ]
    }
   ],
   "source": [
    "neural.compile(loss = \"binary_crossentropy\", optimizer = \"adam\",  metrics = [\"accuracy\"])\n",
    "\n",
    "model = neural.fit(X_train_scaled, y_train, epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ca9b6bd9-359c-4117-87ff-fbb1ebddc74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x29cd4d81b48>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(model.history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5937b8b6-c70c-49df-808c-a66820c7e4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-04-16</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-05-22</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-07-06</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-10-30</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-27</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-08-29</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-05</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-31</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predictions  Actual\n",
       "Date                           \n",
       "2020-04-16            1     1.0\n",
       "2017-05-22            0     1.0\n",
       "2017-11-15            1     0.0\n",
       "2015-07-06            1     0.0\n",
       "2018-10-30            1     1.0\n",
       "...                 ...     ...\n",
       "2017-11-27            1     0.0\n",
       "2018-08-29            1     1.0\n",
       "2019-06-05            0     1.0\n",
       "2015-12-31            1     0.0\n",
       "2019-04-15            1     0.0\n",
       "\n",
       "[1314 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_prediction = (neural.predict(X_train_scaled) > 0.5).astype(\"int32\")\n",
    "Y_prediction = Y_prediction.squeeze()\n",
    "display(Y_prediction)\n",
    "\n",
    "results = pd.DataFrame( {\"Predictions\": Y_prediction, \"Actual\": y_train})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017ecc4-d3c2-4712-9ffe-901f7247dfba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
